{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51cbaf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcd526",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcbdea7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253675</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253676</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253677</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253678</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253679</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253680 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Diabetes_012  HighBP  HighChol  CholCheck  BMI  Smoker  Stroke  \\\n",
       "0                  0       1         1          1   40       1       0   \n",
       "1                  0       0         0          0   25       1       0   \n",
       "2                  0       1         1          1   28       0       0   \n",
       "3                  0       1         0          1   27       0       0   \n",
       "4                  0       1         1          1   24       0       0   \n",
       "...              ...     ...       ...        ...  ...     ...     ...   \n",
       "253675             0       1         1          1   45       0       0   \n",
       "253676             2       1         1          1   18       0       0   \n",
       "253677             0       0         0          1   28       0       0   \n",
       "253678             0       1         0          1   23       0       0   \n",
       "253679             2       1         1          1   25       0       0   \n",
       "\n",
       "        HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
       "0                          0             0       0  ...              1   \n",
       "1                          0             1       0  ...              0   \n",
       "2                          0             0       1  ...              1   \n",
       "3                          0             1       1  ...              1   \n",
       "4                          0             1       1  ...              1   \n",
       "...                      ...           ...     ...  ...            ...   \n",
       "253675                     0             0       1  ...              1   \n",
       "253676                     0             0       0  ...              1   \n",
       "253677                     0             1       1  ...              1   \n",
       "253678                     0             0       1  ...              1   \n",
       "253679                     1             1       1  ...              1   \n",
       "\n",
       "        NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  Age  \\\n",
       "0                 0        5        18        15         1    0    9   \n",
       "1                 1        3         0         0         0    0    7   \n",
       "2                 1        5        30        30         1    0    9   \n",
       "3                 0        2         0         0         0    0   11   \n",
       "4                 0        2         3         0         0    0   11   \n",
       "...             ...      ...       ...       ...       ...  ...  ...   \n",
       "253675            0        3         0         5         0    1    5   \n",
       "253676            0        4         0         0         1    0   11   \n",
       "253677            0        1         0         0         0    0    2   \n",
       "253678            0        3         0         0         0    1    7   \n",
       "253679            0        2         0         0         0    0    9   \n",
       "\n",
       "        Education  Income  \n",
       "0               4       3  \n",
       "1               6       1  \n",
       "2               4       8  \n",
       "3               3       6  \n",
       "4               5       4  \n",
       "...           ...     ...  \n",
       "253675          6       7  \n",
       "253676          2       4  \n",
       "253677          5       2  \n",
       "253678          5       1  \n",
       "253679          6       2  \n",
       "\n",
       "[253680 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"./dataset.csv\")\n",
    "data = data.astype(int)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319cfe6",
   "metadata": {},
   "source": [
    "Features and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd08420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop(['Diabetes_012'], axis=1)\n",
    "y=data['Diabetes_012']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec485894",
   "metadata": {},
   "source": [
    "Let's try backword stepwise elimination (with multinominal logistische regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868b82a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 'PhysActivity' with p = 0.7308\n",
      "Dropping 'DiffWalk' with p = 0.5998\n",
      "Dropping 'Smoker' with p = 0.5562\n",
      "Dropping 'HeartDiseaseorAttack' with p = 0.5250\n",
      "Dropping 'Fruits' with p = 0.4939\n",
      "Dropping 'AnyHealthcare' with p = 0.2242\n",
      "Dropping 'NoDocbcCost' with p = 0.1240\n",
      "Dropping 'Veggies' with p = 0.1182\n",
      "Dropping 'Stroke' with p = 0.0915\n",
      "Selected features: ['const', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'HvyAlcoholConsump', 'GenHlth', 'MentHlth', 'PhysHlth', 'Sex', 'Age', 'Education', 'Income']\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#Add constant for intercept\n",
    "def multinomial(X): \n",
    "    X = sm.add_constant(X)\n",
    "    cols = list(X.columns)\n",
    "    pmax = 1\n",
    "\n",
    "    while len(cols) > 0:\n",
    "        # Inside your while loop:\n",
    "        model = sm.MNLogit(y, X[cols]).fit(disp=0)\n",
    "\n",
    "        # Take max p-value per feature across classes\n",
    "        p_values = model.pvalues\n",
    "        p_values_max = p_values.max(axis=1)\n",
    "        pmax = p_values_max.max()\n",
    "        feature_with_p_max = p_values_max.idxmax()\n",
    "\n",
    "        # Backward elimination step\n",
    "        if pmax > 0.05:\n",
    "            print(f\"Dropping '{feature_with_p_max}' with p = {pmax:.4f}\")\n",
    "            cols.remove(feature_with_p_max)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(f\"Selected features: {cols}\")\n",
    "    return cols\n",
    "\n",
    "X_BSE_list = multinomial(X)\n",
    "X_BSE_list.remove('const')\n",
    "X_postBSE = X[X_BSE_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25386ce",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78faecb",
   "metadata": {},
   "source": [
    "### Assumptions we make while using Decision tree\n",
    "\n",
    "- At the beginning, we consider the whole training set as the root.\n",
    "- Attributes are assumed to be categorical for information gain and for gini index, attributes are assumed to be continuous.\n",
    "- On the basis of attribute values records are distributed recursively.\n",
    "- We use statistical methods for ordering attributes as root or internal node.\n",
    "\n",
    "https://www.geeksforgeeks.org/decision-tree-implementation-python/\n",
    "\n",
    "#### Pseudocode \n",
    "\n",
    "1. Find the best attribute and place it on the root node of the tree.\n",
    "2. Now, split the training set of the dataset into subsets. While making the subset make sure that each subset of training dataset should have the same value for an attribute.\n",
    "3. Find leaf nodes in all branches by repeating 1 and 2 on each subset.\n",
    "\n",
    "#### Decision Trees doesn't need to me standartised.\n",
    "\n",
    "Decision Trees (and related models like Random Forests and Gradient Boosted Trees) are not affected by feature scaling. Here's why:\n",
    "\n",
    "Trees split data based on feature thresholds, not distances or magnitudes.\n",
    "For example, a decision tree might split on \"Feature X > 5\" â€” it doesn't care whether Feature X is in the range [0,1] or [0, 1000].\n",
    "\n",
    "##### When you do need standardization:\n",
    "Standardization (using StandardScaler() or MinMaxScaler()) is important for algorithms that are distance-based or gradient-based, such as:\n",
    "\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Support Vector Machines (SVM)\n",
    "- Logistic Regression\n",
    "- Linear Regression\n",
    "- Neural Networks\n",
    "\n",
    "These models are sensitive to feature scale and can perform poorly if features are not standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28f26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# X hole data Frame\n",
    "# y hale data Frame\n",
    "\"\"\"\n",
    "# === 2. Train/test split === - Janet\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Split training/test - Oliver\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected_rdf, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ==== Train/test split ==== - Noah Light\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_model, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ==== 2. TRAIN/TEST SPLIT ==== - Noah neuronal\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\"\"\"\n",
    "# Function to split the dataset into features and target variables\n",
    "\n",
    "def splitdataset(X, y):\n",
    "\n",
    "    # Splitting the dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \"\"\" How they did in on geeksforgeeks.org:\n",
    "    X_train, X_test, y_train, y_test = train_test_split( \n",
    "          X, Y, test_size = 0.3, random_state = 100)\n",
    "    \"\"\"\n",
    "\n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "def Evaluation(y_test, y_pred, y_pred_proba=None):\n",
    "    \n",
    "    print(\"Accuracy:\", sklearn.metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", sklearn.metrics.precision_score(y_test, y_pred, average='macro'))\n",
    "    print(\"Recall:\", sklearn.metrics.recall_score(y_test, y_pred, average='macro'))\n",
    "    print(\"F1 Score:\", sklearn.metrics.f1_score(y_test, y_pred, average='macro'))\n",
    "    if(y_pred_proba):\n",
    "        print(\"Log Loss:\", sklearn.metrics.log_loss(y_test, y_pred_proba, labels=[0, 1, 2]))\n",
    "    print(\"Cohen Kappa Score:\", sklearn.metrics.cohen_kappa_score(y_test, y_pred))\n",
    "\n",
    "    # Classification report per classe\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ea847",
   "metadata": {},
   "source": [
    "Trying diffrent settings in the DecisionTreeClassifier() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03cf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Variating_DecisionTreeClassifier(Xtrain, ytrain, Xtest, ytest):\n",
    "    class_weight = [\"balanced\",None]\n",
    "    criterions = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "    maxDepth = [2,4,8,16,32,None]\n",
    "\n",
    "    maxf1 = 0\n",
    "    parameters = \"\"\n",
    "\n",
    "    for criterion in criterions:\n",
    "        for d in maxDepth:\n",
    "            for cl in class_weight:\n",
    "            \n",
    "                #print(\"========= Class weigth:\" + str(cl) + \" Method: \" + criterion + \", maxDepth: \" + str(d) + \" ========\")\n",
    "                dtree = DecisionTreeClassifier(criterion=criterion, max_depth=d, class_weight=cl, random_state=42)\n",
    "                dtree.fit(Xtrain, ytrain)\n",
    "                yPred = dtree.predict(Xtest)\n",
    "                if(sklearn.metrics.f1_score(ytest, yPred, average='macro') > maxf1):\n",
    "                    maxf1 = sklearn.metrics.f1_score(ytest, yPred, average='macro')\n",
    "                    parameters = \"Criterion:\" + criterion + \" , depth: \" + str(d) + \", class_weight:\" + str(cl)\n",
    "                    Best_Y_predicted = yPred\n",
    "                #print(\"F1 Score:\", sklearn.metrics.f1_score(y_test, y_predicted, average='macro'))\n",
    "\n",
    "    print(\"Best result: \" + str(maxf1) + \" with parameter \" + parameters)\n",
    "\n",
    "    return Best_Y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc143f",
   "metadata": {},
   "source": [
    "Testing on hole Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221da0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7673446862188584\n",
      "F1: 0.7740744661185558\n",
      "accuracy: 0.8484113844213182\n",
      "F1: 0.7968889359217897\n",
      "Accuracy: 0.8484113844213182\n",
      "Precision: 0.480702274892619\n",
      "Recall: 0.3626976714253866\n",
      "F1 Score: 0.36189251218846263\n",
      "Cohen Kappa Score: 0.12275569974485112\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8546    0.9898    0.9172     42795\n",
      "           1     0.0000    0.0000    0.0000       944\n",
      "           2     0.5875    0.0983    0.1685      6997\n",
      "\n",
      "    accuracy                         0.8484     50736\n",
      "   macro avg     0.4807    0.3627    0.3619     50736\n",
      "weighted avg     0.8018    0.8484    0.7969     50736\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noemi\\PyMOL\\envs\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\noemi\\PyMOL\\envs\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\noemi\\PyMOL\\envs\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\noemi\\PyMOL\\envs\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' ploting desicion tree\\nplt.figure(figsize=(40, 10))\\nsklearn.tree.plot_tree(dtree, feature_names=newList)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing first on the hole data set\n",
    "X_train, X_test, y_train, y_test = splitdataset(X,y)\n",
    "#DataSetSplit_Reduced = splitdataset(X_reducedRelevant,y)\n",
    "\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "dtree = dtree.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = dtree.predict(X_test)\n",
    "Accurat = accuracy_score(y_test, y_predicted)\n",
    "F1 = f1_score(y_test, y_predicted, average='weighted')\n",
    "print(f\"accuracy: {Accurat}\")\n",
    "print(f\"F1: {F1}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = splitdataset(X_postBSE,y)\n",
    "\n",
    "dtree = DecisionTreeClassifier(max_depth=5)\n",
    "dtree = dtree.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = dtree.predict(X_test)\n",
    "Accurat = accuracy_score(y_test, y_predicted)\n",
    "F1 = f1_score(y_test, y_predicted, average='weighted')\n",
    "print(f\"accuracy: {Accurat}\")\n",
    "print(f\"F1: {F1}\")\n",
    "\n",
    "Evaluation(y_test, y_predicted)\n",
    "\"\"\" ploting desicion tree\n",
    "plt.figure(figsize=(40, 10))\n",
    "sklearn.tree.plot_tree(dtree, feature_names=newList)\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17ab5f",
   "metadata": {},
   "source": [
    "with backword stepwise elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4b58225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best result: 0.41796734327242274 with parameter Criterion:gini , depth: 4, class_weight:balanced\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = splitdataset(X_postBSE,y)\n",
    "\n",
    "dtree = DecisionTreeClassifier(max_depth=5)\n",
    "dtree = dtree.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = dtree.predict(X_test)\n",
    "Accurat = accuracy_score(y_test, y_predicted)\n",
    "F1 = f1_score(y_test, y_predicted, average='weighted')\n",
    "print(f\"accuracy: {Accurat}\")\n",
    "print(f\"F1: {F1}\")\n",
    "\n",
    "Evaluation(y_test, y_predicted)\n",
    "\"\"\"\n",
    "\"\"\" \n",
    "ploting desicion tree\n",
    "plt.figure(figsize=(40, 10))\n",
    "sklearn.tree.plot_tree(dtree, feature_names=newList)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# split reduced data set\n",
    "X_train, X_test, y_train, y_test = splitdataset(X_postBSE,y)\n",
    "\n",
    "_ = Variating_DecisionTreeClassifier(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76665f5",
   "metadata": {},
   "source": [
    "Using SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e22a87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best result: 0.39444583043283377 with parameter Criterion:gini , depth: 8, class_weight:balanced\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# split hole data set\n",
    "X_train, X_test, y_train, y_test = splitdataset(X,y)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "_ = Variating_DecisionTreeClassifier(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21917947",
   "metadata": {},
   "source": [
    "Using random oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36d5b0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling: Counter({0: 170908, 2: 28349, 1: 3687})\n",
      "Resampled dataset: Counter({0: 170908, 2: 170908, 1: 170908})\n",
      "Best result: 0.4222715185349686 with parameter Criterion:gini , depth: 4, class_weight:balanced\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "# split hole data set\n",
    "X_train, X_test, y_train, y_test = splitdataset(X,y)\n",
    "\n",
    "# Check class distribution before oversampling\n",
    "print(\"Before oversampling:\", Counter(y_train))\n",
    "\n",
    "ros = RandomOverSampler(random_state=40)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Resampled dataset:\", Counter(y_resampled))\n",
    "\n",
    "Best_Y_predicted = Variating_DecisionTreeClassifier(X_resampled, y_resampled, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae8e86",
   "metadata": {},
   "source": [
    "Using random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71435493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f610a311",
   "metadata": {},
   "source": [
    "Backward stepwise elimination and random oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5bd5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling: Counter({0: 170908, 2: 28349, 1: 3687})\n",
      "Resampled dataset: Counter({0: 170908, 2: 170908, 1: 170908})\n",
      "Best result: 0.41796734327242274 with parameter Criterion:gini , depth: 4, class_weight:balanced\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = splitdataset(X_postBSE,y)\n",
    "\n",
    "# Check class distribution before oversampling\n",
    "print(\"Before oversampling:\", Counter(y_train))\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Resampled dataset:\", Counter(y_resampled))\n",
    "\n",
    "Best_Y_predicted_postBSE = Variating_DecisionTreeClassifier(X_resampled, y_resampled, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60ef2a",
   "metadata": {},
   "source": [
    "Detailed results for the best F1 score found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e13519a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6694654683065279\n",
      "Precision: 0.42908045980383913\n",
      "Recall: 0.4977856172826973\n",
      "F1 Score: 0.4222715185349686\n",
      "Cohen Kappa Score: 0.2476613742558187\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9391    0.6883    0.7944     42795\n",
      "           1     0.0306    0.1854    0.0526       944\n",
      "           2     0.3175    0.6197    0.4199      6997\n",
      "\n",
      "    accuracy                         0.6695     50736\n",
      "   macro avg     0.4291    0.4978    0.4223     50736\n",
      "weighted avg     0.8365    0.6695    0.7289     50736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" from Oliver\n",
    "# Prediction\n",
    "y_pred = rdf.predict(X_test)\n",
    "y_pred_proba = rdf.predict_proba(X_test)\n",
    "\"\"\"\n",
    "\n",
    "Evaluation(y_test, Best_Y_predicted)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, Best_Y_predicted, labels=[0, 1, 2])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig(\"Output/Confusion Matrix of Decision Tree.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cd87d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.41796734327242274\n"
     ]
    }
   ],
   "source": [
    "# Plot the decision tree\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "\n",
    "dtree = DecisionTreeClassifier(criterion=\"gini\", max_depth=4, class_weight=\"balanced\", random_state=42)\n",
    "dtree.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_predicted = dtree.predict(X_test)\n",
    "f1 = sklearn.metrics.f1_score(y_test, y_predicted, average='macro')\n",
    "print(\"F1 Score:\", sklearn.metrics.f1_score(y_test, y_predicted, average='macro'))\n",
    "\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plot_tree(dtree, filled=True, feature_names=load_iris().feature_names, class_names=load_iris().target_names)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "#ploting desicion tree\n",
    "plt.figure(figsize=(80, 40))\n",
    "sklearn.tree.plot_tree(dtree, filled=True, feature_names=X.columns, proportion=True)\n",
    "plt.savefig(\"output/high_res_tree.png\", dpi=500, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff222248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#training a decision tree classifier using the Gini index as the splitting criterion\\ndef train_using_gini(X_train, X_test, y_train):\\n\\n    # Creating the classifier object\\n    clf_gini = DecisionTreeClassifier(criterion=\"gini\",\\n                                      random_state=42, max_depth=3, min_samples_leaf=5)\\n\\n    # Performing training\\n    clf_gini.fit(X_train, y_train)\\n    return clf_gini\\n\\n\\ndef train_using_entropy(X_train, X_test, y_train):\\n\\n    # Decision tree with entropy\\n    clf_entropy = DecisionTreeClassifier(\\n        criterion=\"entropy\", random_state=42,\\n        max_depth=3, min_samples_leaf=5)\\n\\n    # Performing training\\n    clf_entropy.fit(X_train, y_train)\\n    return clf_entropy\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#training a decision tree classifier using the Gini index as the splitting criterion\n",
    "def train_using_gini(X_train, X_test, y_train):\n",
    "\n",
    "    # Creating the classifier object\n",
    "    clf_gini = DecisionTreeClassifier(criterion=\"gini\",\n",
    "                                      random_state=42, max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "    # Performing training\n",
    "    clf_gini.fit(X_train, y_train)\n",
    "    return clf_gini\n",
    "\n",
    "\n",
    "def train_using_entropy(X_train, X_test, y_train):\n",
    "\n",
    "    # Decision tree with entropy\n",
    "    clf_entropy = DecisionTreeClassifier(\n",
    "        criterion=\"entropy\", random_state=42,\n",
    "        max_depth=3, min_samples_leaf=5)\n",
    "\n",
    "    # Performing training\n",
    "    clf_entropy.fit(X_train, y_train)\n",
    "    return clf_entropy\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
